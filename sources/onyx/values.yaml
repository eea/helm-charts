# Default values for onyx.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  # Global version for all Onyx components (overrides .Chart.AppVersion)
  version: "v2.3.0-eea.0.0.83-dev"
  # Optional global image settings. If repository is set, all core components default to this repo.
  # Component-specific tags can be set under tags; fall back to per-component tags or global.version.
  image:
    repository: "eeacms/danswer"
    tags:
      backend: ""
      web: ""
      modelServer: ""
  # Global pull policy for all Onyx component images
  pullPolicy: "IfNotPresent"
  # Global storage class override applied where component-specific storageClass is unset
  storageClass: "nfs-client"
  # Add Helm keep annotation to PVCs managed by this chart
  pvcRetain: true

postgresql:
  enabled: true
  # Set to false to use an external Postgres instance instead of deploying one
  external:
    host: ""
    port: 5432
  # Simple StatefulSet configuration (default)
  simple:
    tag: "16"
    database: "onyx"
    storage: "10Gi"
    storageClass: ""
    # Add Helm keep policy to retain the PVC on uninstall
    retainPvc: true
    # Use an existing PVC instead of creating one (skips volumeClaimTemplates)
    existingClaim: ""
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2000m
        memory: 4Gi
    securityContext:
      runAsNonRoot: false
      runAsUser: 999
  # Database initialization configuration
  # Creates additional databases and users on PostgreSQL startup
  initdb:
    enabled: false
    databases: []
    # Example configuration:
    # databases:
    #   - name: myapp_db          # Database name
    #     user: myapp_user        # Database user
    #     password: changeme      # User password
    #     createSecret: true      # Auto-create Kubernetes secret with connection details

  # Backup configuration
  backup:
    enabled: false
    schedule: "0 2 * * *"  # Daily at 2 AM (cron format)
    storage: "10Gi"
    storageClass: ""
    retainPvc: true
    successfulJobsHistoryLimit: 3
    failedJobsHistoryLimit: 3
    retention:
      enabled: true
      days: 7  # Keep backups for 7 days
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Restore configuration
  restore:
    enabled: false
    # Specify one of: backupFile, timestamp, or leave empty for latest
    backupFile: ""  # Full path: /backups/onyx_20240101_120000.sql.gz
    timestamp: ""   # Format: 20240101_120000
    database: "onyx"  # Database to restore
    dropDatabase: false  # Drop existing database before restore (DANGER!)
    createDatabase: true  # Create database if it doesn't exist
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

vespa:
  name: da-vespa-0
  service:
    name: vespa-service
  volumeClaimTemplates:
    - metadata:
        name: vespa-storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
        storageClassName: ""
  enabled: true
  replicaCount: 1
  image:
    repository: vespa
    tag: "8.609.39"
  podAnnotations: {}
  podLabels:
    app: vespa
    app.kubernetes.io/instance: onyx
    app.kubernetes.io/name: vespa
  securityContext:
    privileged: false
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL
  resources:
    # The Vespa Helm chart specifies default resources, which are quite modest. We override
    # them here to increase chances of the chart running successfully. If you plan to index at
    # scale, you will likely need to increase these limits further.
    # At large scale, it is recommended to use a dedicated Vespa cluster / Vespa cloud.
    requests:
      cpu: 4000m
      memory: 8000Mi
    limits:
      cpu: 8000m
      memory: 32000Mi

persistent:
  storageClassName: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

autoscaling:
  # Valid options: 'hpa' (default) or 'keda'.
  # Set to 'keda' to render KEDA ScaledObjects for components that have autoscaling enabled.
  # When using KEDA you must install and manage the KEDA operator separately; it is not bundled with this chart.
  engine: hpa

inferenceCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  name: inference-model-server
  replicaCount: 1
  labels:
    - key: app
      value: inference-model-server
  containerPorts:
    server: 9000
  podLabels:
    - key: app
      value: inference-model-server
  resources:
    requests:
      cpu: 2000m
      memory: 3Gi
    limits:
      cpu: 4000m
      memory: 10Gi
  podSecurityContext: {}
  securityContext:
    privileged: false
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL
  nodeSelector: {}
  volumes:
    - name: cache
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: logs
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx  
    - name: cache
      mountPath: /.cache
    - name: tmp
      mountPath: /tmp
  tolerations: []
  affinity: {}

indexCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  replicaCount: 1
  name: indexing-model-server
  deploymentLabels:
    app: indexing-model-server
  podLabels:
    app: indexing-model-server
  indexingOnly: "True"
  podAnnotations: {}
  containerPorts:
    server: 9000
  limitConcurrency: 10
  resources:
    requests:
      cpu: 4000m
      memory: 3Gi
    limits:
      cpu: 6000m
      memory: 6Gi
  podSecurityContext: {}
  securityContext:
    privileged: false
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL
  nodeSelector: {}
  volumes:
    - name: cache
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: logs
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx  
    - name: cache
      mountPath: /.cache
    - name: tmp
      mountPath: /tmp
  tolerations: []
  affinity: {}
config:
  envConfigMapName: env-configmap

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

nginx:
  enabled: true
  # Namespace-scoped nginx deployed by this chart
  useSimpleDeployment: true
  # Nginx proxy timeout settings (in seconds)
  timeouts:
    connect: 300  # Time to establish connection with upstream server
    send: 300     # Time to send request to upstream server
    read: 300     # Time to read response from upstream server
  # Simple Deployment configuration (when useSimpleDeployment: true)
  simple:
    tag: "alpine"
    replicas: 1
    serviceType: "ClusterIP"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    securityContext:
      runAsNonRoot: true
      runAsUser: 101
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
  # Ingress configuration (when useSimpleDeployment: true)
  ingress:
    enabled: false
    className: "nginx"
    annotations: {}
      # cert-manager.io/cluster-issuer: "letsencrypt-prod"
    hosts:
      - host: onyx.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
      # - secretName: onyx-tls-cert
      #   hosts:
      #     - onyx.example.com

webserver:
  replicaCount: 1
  deploymentLabels:
    app: web-server
  podAnnotations: {}
  podLabels:
    app: web-server
  podSecurityContext:
    fsGroup: 2000

  securityContext:
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  containerPorts:
    server: 3000

  service:
    type: ClusterIP
    servicePort: 3000
    targetPort: http

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache

  nodeSelector: {}
  tolerations: []
  affinity: {}

api:
  replicaCount: 1
  deploymentLabels:
    app: api-server
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
    app: api-server

  containerPorts:
    server: 8080

  podSecurityContext:
    fsGroup: 2000

  securityContext:
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  service:
    type: ClusterIP
    servicePort: 8080
    targetPort: api-server-port
    portName: api-server-port

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 3Gi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache

  nodeSelector: {}
  tolerations: []
  affinity: {}


######################################################################
#
# Background workers
#
######################################################################

celery_shared:
  startupProbe:
    # startupProbe fails after 2m
    exec:
      command: ["test", "-f", "/app/onyx/main.py"]
    failureThreshold: 24
    periodSeconds: 5
    timeoutSeconds: 3
  readinessProbe:
    # readinessProbe fails after 15s + 2m of inactivity
    # it's ok to see the readinessProbe fail transiently while the container starts
    initialDelaySeconds: 15
    periodSeconds: 5
    failureThreshold: 24
    timeoutSeconds: 3
  livenessProbe:
    # livenessProbe fails after 5m of inactivity
    initialDelaySeconds: 60
    periodSeconds: 60
    failureThreshold: 5
    timeoutSeconds: 3
  podSecurityContext: {}
  securityContext:
    privileged: false
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL

celery_beat:
  replicaCount: 1
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-beat
  deploymentLabels:
    app: celery-beat
  persistence:
    enabled: false
    size: 1Gi
    storageClass: ""
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
    - name: data
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
    - name: data
      mountPath: /app/data
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_heavy:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-heavy
  deploymentLabels:
    app: celery-worker-heavy
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_docprocessing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-docprocessing
  deploymentLabels:
    app: celery-worker-docprocessing
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 12Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_light:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-light
  deploymentLabels:
    app: celery-worker-light
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 4Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_monitoring:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-monitoring
  deploymentLabels:
    app: celery-worker-monitoring
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_primary:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-primary
  deploymentLabels:
    app: celery-worker-primary
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_user_files_indexing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-user-files-indexing
  deploymentLabels:
    app: celery-worker-user-files-indexing
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_user_file_processing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-user-file-processing
  deploymentLabels:
    app: celery-worker-user-file-processing
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

slackbot:
  enabled: true
  replicaCount: 1
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
    app: slack-bot
  deploymentLabels:
    app: slack-bot
  podSecurityContext:
    fsGroup: 2000
  securityContext:
    capabilities:
      drop:
      - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "2000Mi"
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_docfetching:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-docfetching
  deploymentLabels:
    app: celery-worker-docfetching
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 16Gi
  volumes:
    - name: logs
      emptyDir: {}
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
  volumeMounts:
    - name: logs
      mountPath: /var/log/onyx
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /.cache
  nodeSelector: {}
  tolerations: []
  affinity: {}

######################################################################
#
# End background workers section
#
######################################################################

redis:
  enabled: true
  useSimpleDeployment: true
  simple:
    tag: "7.0-alpine"
    storage: "1Gi"
    storageClass: ""
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    securityContext:
      runAsNonRoot: true
      runAsUser: 999
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
  service:
    port: 6379
  securityContext:
    runAsNonRoot: true
    runAsUser: 999
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL
  podSecurityContext:
    fsGroup: 2000

minio:
  enabled: true
  mode: standalone
  replicas: 1
  drivesPerNode: 1
  existingSecret: onyx-objectstorage
  buckets:
    - name: onyx-file-store-bucket
  persistence:
    enabled: true
    size: 30Gi
    storageClass: ""
  service:
    type: ClusterIP
    port: 9000
  consoleService:
    type: ClusterIP
    port: 9001
  nodeSelector: {}
  tolerations: []
  affinity: {}

ingress:
  enabled: false
  className: ""
  api:
    host: onyx.local
  webserver:
    host: onyx.local

letsencrypt:
  enabled: false
  email: "abc@abc.com"

# -- Governs all Secrets created or used by this chart. Values set by this chart will be base64 encoded in the k8s cluster.
auth:
  postgresql:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-postgresql'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      # CloudNativePG requires `username` and `password` keys for the superuser secret.
      POSTGRES_USER: username
      POSTGRES_PASSWORD: password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      username: "postgres"
      password: "postgres"
  redis:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-redis'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      REDIS_PASSWORD: redis_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      redis_password: "password"
  objectstorage:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-objectstorage'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      S3_AWS_ACCESS_KEY_ID: s3_aws_access_key_id
      S3_AWS_SECRET_ACCESS_KEY: s3_aws_secret_access_key
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      s3_aws_access_key_id: "minioadmin"
      s3_aws_secret_access_key: "minioadmin"
      rootUser: "minioadmin"
      rootPassword: "minioadmin"
  oauth:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-oauth'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      OAUTH_CLIENT_ID: "oauth_client_id"
      OAUTH_CLIENT_SECRET: "oauth_client_secret"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      oauth_client_id: ""
      oauth_client_secret: ""
  smtp:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-smtp'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      SMTP_PASS: "smtp_pass"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      smtp_pass: ""
  dbreadonly:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-dbreadonly'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      DB_READONLY_USER: db_readonly_user
      DB_READONLY_PASSWORD: db_readonly_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      db_readonly_user: ""
      db_readonly_password: ""

configMap:
  # Change this for production uses unless Onyx is only accessible behind VPN
  AUTH_TYPE: "disabled"
  # 1 Day Default
  SESSION_EXPIRE_TIME_SECONDS: "86400"
  # Can be something like onyx.app, as an extra double-check
  VALID_EMAIL_DOMAINS: ""
  # For sending verification emails, true or false
  REQUIRE_EMAIL_VERIFICATION: ""
  # If unspecified then defaults to 'smtp.gmail.com'
  SMTP_SERVER: ""
  # For sending verification emails, if unspecified then defaults to '587'
  SMTP_PORT: ""
# 'your-email@company.com'
  SMTP_USER: ""
  # 'your-gmail-password'
  # SMTP_PASS: ""
  # 'your-email@company.com' SMTP_USER missing used instead
  EMAIL_FROM: ""
  # MinIO/S3 Configuration override
  S3_ENDPOINT_URL: ""  # only used if minio is not enabled
  S3_FILE_STORE_BUCKET_NAME: ""
  # Gen AI Settings
  GEN_AI_MAX_TOKENS: ""
  QA_TIMEOUT: "60"
  MAX_CHUNKS_FED_TO_CHAT: ""
  DISABLE_LLM_DOC_RELEVANCE: ""
  DISABLE_LLM_CHOOSE_SEARCH: ""
  DISABLE_LLM_QUERY_REPHRASE: ""
  # Query Options
  DOC_TIME_DECAY: ""
  HYBRID_ALPHA: ""
  EDIT_KEYWORD_QUERY: ""
  MULTILINGUAL_QUERY_EXPANSION: ""
  LANGUAGE_HINT: ""
  LANGUAGE_CHAT_NAMING_HINT: ""
  # Don't change the NLP models unless you know what you're doing
  EMBEDDING_BATCH_SIZE: ""
  DOCUMENT_ENCODER_MODEL: ""
  NORMALIZE_EMBEDDINGS: ""
  ASYM_QUERY_PREFIX: ""
  ASYM_PASSAGE_PREFIX: ""
  DISABLE_RERANK_FOR_STREAMING: ""
  MODEL_SERVER_PORT: ""
  MIN_THREADS_ML_MODELS: ""
  # Indexing Configs
  VESPA_SEARCHER_THREADS: ""
  NUM_INDEXING_WORKERS: ""
  DISABLE_INDEX_UPDATE_ON_SWAP: ""
  DASK_JOB_CLIENT_ENABLED: ""
  CONTINUE_ON_CONNECTOR_FAILURE: ""
  EXPERIMENTAL_CHECKPOINTING_ENABLED: ""
  CONFLUENCE_CONNECTOR_LABELS_TO_SKIP: ""
  JIRA_CLOUD_API_VERSION: ""
  JIRA_SERVER_API_VERSION: ""
  GONG_CONNECTOR_START_TIME: ""
  NOTION_CONNECTOR_ENABLE_RECURSIVE_PAGE_LOOKUP: ""
  # Worker Parallelism
  CELERY_WORKER_DOCPROCESSING_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_PREFETCH_MULTIPLIER: ""
  CELERY_WORKER_USER_FILE_PROCESSING_CONCURRENCY: ""
  # OnyxBot SlackBot Configs
  ONYX_BOT_DISABLE_DOCS_ONLY_ANSWER: ""
  ONYX_BOT_DISPLAY_ERROR_MSGS: ""
  ONYX_BOT_RESPOND_EVERY_CHANNEL: ""
  NOTIFY_SLACKBOT_NO_ANSWER: ""
  # Logging
  # Optional Telemetry, please keep it on (nothing sensitive is collected)? <3
  DISABLE_TELEMETRY: ""
  LOG_LEVEL: ""
  LOG_ALL_MODEL_INTERACTIONS: ""
  LOG_ONYX_MODEL_INTERACTIONS: ""
  LOG_VESPA_TIMING_INFORMATION: ""
  # Shared or Non-backend Related
  WEB_DOMAIN: "http://localhost:3000"
  # DOMAIN used by nginx
  DOMAIN: "localhost"
  # Chat Configs
  HARD_DELETE_CHATS: ""
  # User File Upload Configuration
  # Skip the token count threshold check (100,000 tokens) for uploaded files
  # For self-hosted: set to true to skip for all users
  SKIP_USERFILE_THRESHOLD: ""
  # For multi-tenant: comma-separated list of tenant IDs to skip threshold
  SKIP_USERFILE_THRESHOLD_TENANT_IDS: ""

# Postfix mail relay configuration
postfix:
  enabled: false
  dryrun: false
  serverName: ""
  mtpHost: ""
  mtpPort: 587
  mtpRelay: ""
  mtpUser: ""
  mtpPass: ""
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
  affinity: {}
  tolerations: []
  mailtrap:
    httpenabled: false
    ingress:
      enabled: false
    mtPasswd: ""
    mtUser: ""
    serviceType: ClusterIP

# Network Policy configuration
networkPolicy:
  # Enable network policies to restrict traffic between pods
  enabled: false
  # Optional: specify namespaces that can access ingress
  # ingressFromNamespaces:
  #   environment: production
